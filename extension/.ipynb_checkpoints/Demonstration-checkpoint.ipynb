{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration: The MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset\n",
    "\n",
    "The script ``mnist.py`` can read MNIST data. The options ``vectorize`` converts it into a usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mnist'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4d1106261b01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mnist'"
     ]
    }
   ],
   "source": [
    "import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = mnist.read(path='mnist', dataset='training', vectorize=True)\n",
    "testing_set = mnist.read(path='mnist', dataset='testing', vectorize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing and training the network\n",
    "\n",
    "We can now build the network. We'll use 784 input nodes, one for each pixel, and 10 output nodes, one for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = pynn.Network([784, 10], activation='ReLU', softmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use ReLU as the activation function and apply a softmax filter since this is a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(training_set, epochs=1, batchsize=1, eta=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting test samples\n",
    "\n",
    "Now that the network is trained we can try to predict a sample from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = network.predict(testing_set[42][0])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be a 4. Let's check by showing the actual image alongside with it's label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mnist.show(np.array(testing_set[42][0]).reshape(28, 28))\n",
    "print(testing_set[42][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the network was correct. But what is our actual error rate over the whole testing set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_wrong = 0\n",
    "for sample in testing_set:\n",
    "    prediction = network.predict(sample[0])\n",
    "    prediction = np.argmax(prediction)\n",
    "    expectation = np.argmax(sample[1])\n",
    "    if not prediction == expectation:\n",
    "        num_wrong += 1\n",
    "error_rate = num_wrong / len(testing_set)\n",
    "print(\"Achieved testing error rate: %.2f %%.\" % (error_rate * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 9 %? That's pretty good. In fact, it's better than what LeCun et al. achieved 1998 using a similar setup. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for more epochs\n",
    "\n",
    "Below is a graph illustrating the evolution of the testing and training errors when training for up to 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data = np.loadtxt(\"data.txt\")\n",
    "plt.plot(data[:, 0], data[:, 2] * 100, label=\"Testing set\")\n",
    "plt.plot(data[:, 0], data[:, 1] * 100, linestyle=\"dashed\", label=\"Training set\")\n",
    "plt.xlim(0, 50)\n",
    "plt.ylim(6, 10)\n",
    "plt.grid(linestyle=\"dashed\", color=\"grey\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(r\"Error rate in %\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Y. LeCun, L. Bottou, Y. Bengio and P. Haffner: Gradient-Based Learning Applied to Document Recognition, Proceedings of the IEEE, 86(11):2278-2324, November 1998"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
