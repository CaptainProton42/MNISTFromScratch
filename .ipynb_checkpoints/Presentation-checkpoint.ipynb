{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MNIST from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The goal of this project is to:\n",
    "* write a machine learning algorithm from scratch\n",
    "* apply this algorithm to a training set, in this case MNIST [2]\n",
    "\n",
    "I expanded this scope a bit by also creating my own <i>MNIST-style</i> training set and applying the algorithm to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which algorithm should we use?\n",
    "\n",
    "\\begin{array}{lrr} \\hline\n",
    "\\textbf{classifier} & \\textbf{test error rate} & \\textbf{reference} \\\\ \\hline\n",
    "\\text{linear classifier (1-layer NN)} & 12.0 \\% & \\text{LeCun et. al. 1998} \\\\ \\hline\n",
    "\\text{K-nearest-neighbors, L3} & 2.83 \\% & \\text{Kenneth Wilder, U. Chicago} \\\\ \\hline\n",
    "\\text{2-layer NN, 800 HU, Cross-Entropy Loss} & 1.6 \\% & \\text{Simard et al., ICDAR 2003}  \\\\ \\hline\n",
    "\\text{deep conv. net, 7 HL [elastic distortions]} & 0.35 \\% &  \\text{Ciresan et al. IJCAI 2011} \\\\ \\hline\n",
    "\\end{array}\n",
    "\n",
    "<center><a>http://yann.lecun.com/exdb/mnist/</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* (Multi-layer) perceptron:\n",
    "    * low error rates (1.6 %)\n",
    "    * \"vanilla\" neural network\n",
    "    * easy to implement and understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The network\n",
    "\n",
    "* fully connected feed-forward network (perceptron)\n",
    "* sigmoid, tanh, (Leaky) ReLU or linear activation\n",
    "* squared error funtion\n",
    "* optional softmax layer with cross entropy loss\n",
    "* gradient descent with arbitrary batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"media/img/neuralnet.png\" alt=\"lol wo ist das bild\" width=\"70%\"/>\n",
    "(based on http://www.texample.net/tikz/examples/neural-network/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Writing the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Programming Language\n",
    "\n",
    "C extension for Python:\n",
    "* no external libraries (standard C)\n",
    "* compiled code\n",
    "\n",
    "In Python:\n",
    "* import of the `pyceptron` module\n",
    "* handling of `Network` objects in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tools\n",
    "\n",
    "* Anaconda 3 Python 3 installation\n",
    "* <b>Visual Studio Code</b>\n",
    "    * Open source code editor by Microsoft many available plugins\n",
    "    * https://github.com/Microsoft/vscode\n",
    "* Micrsoft Visual C++ (MSVC)\n",
    "* Qt Designer\n",
    "* Spyder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview: Network Object\n",
    "\n",
    "```Python\n",
    "Network(architecture, activation=\"sigmoid\", softmax=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```Python\n",
    "    train(dataset, epochs, batchsize, eta)\n",
    "    \n",
    "    predict(input_vector)\n",
    "    \n",
    "    set_weight(l, j, k, new_weight)\n",
    "    set_bias(l, j, new_bias)\n",
    "    \n",
    "    get_weight(l, j, k)\n",
    "    get_bias(l, j)\n",
    "    \n",
    "    save_state(file_path)\n",
    "    load_state(file_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* How should our `Network` object be represented in code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `Network` Structure\n",
    "\n",
    "```C\n",
    "typedef struct {\n",
    "    PyObject_HEAD /* declares a Python type */\n",
    "\n",
    "    Layer *layers; /* array of layer objects */\n",
    "\n",
    "    int numLayers; /* number of layers */\n",
    "    int *numNeurons; /* number of neurons in each layer */\n",
    "\n",
    "    double (*activationFunc)(double); /* activation function */\n",
    "    double (*activationFuncGradient)(double); /* gradient of activation function */\n",
    "\n",
    "    int softmax;    /* enable softmax? */\n",
    "} NetworkObject;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```C\n",
    "typedef struct layerStruct {\n",
    "    Node *nodes; /* Nodes in layer */\n",
    "\n",
    "    void (*initLayer)(struct layerStruct *self,    /* layer initialization */\n",
    "                      int numberOfNodes,\n",
    "                      int numberOfPreviousNodes);\n",
    "} Layer;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```C\n",
    "typedef struct nodeStruct {\n",
    "    double o; /* Node output */\n",
    "\n",
    "    double *w; /* Input weights */\n",
    "    double *grad_w; /* Input weight gradients */\n",
    "    double b; /* Input bias */\n",
    "    double grad_b; /* Input bias gradient */\n",
    "\n",
    "    void (*initNode)(struct nodeStruct *self, int numberOfPreviousNodes); /* node initialization */\n",
    "} Node;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `Network.train`\n",
    "\n",
    "```C\n",
    "for ( sampleInBatch = 0; sampleInBatch < batchsize; sampleInBatch++ )\n",
    "{\n",
    "    sample = sampleInBatch + batch * batchsize; // batch offset\n",
    "    ...\n",
    "    forwardfeed(self);\n",
    "    if (self->softmax) softmax(self);\n",
    "    backpropagation(self, outputs[sample]);\n",
    "}\n",
    "...\n",
    "apply gradients\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `void forwardfeed(NetworkObject *self)`\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "o_j^l = \\sigma \\left( \\sum_{k=0}^{n_{l-1} - 1} o_k^{l-1} w_{jk}^l + b_j^l \\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```C\n",
    "int l, j;\n",
    "for ( l = 1; l < self->numLayers; l++ ) /* loop through layers */\n",
    "{\n",
    "    for ( j = 0; j < self->numNeurons[l]; j++ ) /* loop through neurons */\n",
    "    {          \n",
    "        double z = 0.0; /* net input */\n",
    "        int i;\n",
    "        for ( i = 0; i < self->numNeurons[l-1]; i++ ) /* multiply outputs and weights */\n",
    "        {\n",
    "            z += self->layers[l].nodes[j].w[i] * self->layers[l-1].nodes[i].o;\n",
    "        }\n",
    "        z += self->layers[l].nodes[j].b; /* add bias */\n",
    "        self->layers[l].nodes[j].o = self->activationFunc(z); /* apply activation */\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `double (*activationFunc)(double)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 3;\">\n",
    "<div style=\"width:15%; display: inline-block;\">\n",
    "sigmoid:\n",
    "</div>\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma (x) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma'(x) = \\sigma(x) ( 1 - \\sigma(x))\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 2;\">\n",
    "<div style=\"width:15%; display: inline-block;\">\n",
    "tanh:\n",
    "</div>\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma (x) = \\frac{1 + \\tanh(z)}{2}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma'(x) = \\frac{1 - \\sigma(x)^2}{2}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 2;\">\n",
    "<div style=\"width:15%; display: inline-block;\">\n",
    "ReLU:\n",
    "</div>\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma (x) =\n",
    "\\begin{cases}\n",
    "    x, & \\text{if}\\ x > 0 \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma'(x) =\n",
    "\\begin{cases}\n",
    "    1, & \\text{if}\\ x > 0 \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 2;\">\n",
    "<div style=\"width:15%; display: inline-block;\">\n",
    "Leaky ReLU:\n",
    "</div>\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma (x) =\n",
    "\\begin{cases}\n",
    "    x, & \\text{if}\\ x > 0 \\\\\n",
    "    0.01x, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma'(x) =\n",
    "\\begin{cases}\n",
    "    1, & \\text{if}\\ x > 0 \\\\\n",
    "    0.01, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 2;\">\n",
    "<div style=\"width:15%; display: inline-block;\">\n",
    "linear:\n",
    "</div>\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma (x) = x\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma'(x) = 1\n",
    "\\end{align*}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `backpropagation(NetworkObject *self, double *output);`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Squared Error Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The squared error loss is defined as\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "E = \\frac{1}{2} \\sum_k ( y_k - o_k )^2 \n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "where $\\mathbf{y}$ is our target and $\\mathbf{o}$ is our predicted output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The derivative is easy:\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial o_i} = o_i - y_i\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Softmax and Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Softmax normalizes an unnormalized vector into a probability distribution.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "p_j = \\frac{e^{o_j}}{\\sum_{k=0}^{n_j} e^{o_k}}\n",
    "\\end{align*}\n",
    "$\n",
    "for $j = 1,\\dots,n_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be9c1fb026f474084521c6f25155f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(FloatSlider(value=0.0, description='Apple', layout=Layout(width='90%'), max=5.0,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from media.softmax_widget import *\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```C\n",
    "void softmax(NetworkObject *self)\n",
    "{\n",
    "    ...\n",
    "    for ( j = 0; j < self->numNeurons[l]; j++)\n",
    "    {\n",
    "        z += exp(self->layers[l].nodes[j].o);\n",
    "    }\n",
    "\n",
    "    for ( j = 0; j < self->numNeurons[l]; j++)\n",
    "    {\n",
    "        self->layers[l].nodes[j].o = exp(self->layers[l].nodes[j].o) / z;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Entropy\n",
    "\n",
    "* minimum number of bits needed to encode a known distribution $y_i$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "H(y) = -\\sum_i y_i \\log(y_i)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Cross Entropy\n",
    "\n",
    "* number of bits needed to encode a distribution $y_i$ using an incorrect distribution $p_i$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "H(y,p) = - \\sum_i y_i \\log(p_i)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want to <b>minimize</b> this <b>loss function</b> so as to reach the optimal encoding $p_i = y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient of Cross Entropy Loss\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial H}{\\partial o_i} &= \\sum_k \\frac{\\partial H}{\\partial p_k} \\cdot \\frac{\\partial p_k}{\\partial o_i} = -\\sum_k y_k \\frac{1}{p_k} \\cdot \\frac{\\partial p_k}{\\partial o_i}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using the derivative\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial p_k}{\\partial a_j} = p_k ( \\delta_{ki} - p_i )\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "of the softmax function we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial H}{\\partial o_i} = p_i - y_i\n",
    "\\end{align*}\n",
    "$.\n",
    "\n",
    "* same as loss gradient of the squared error function using the output of the softmax layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 2;\">\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial w^l_{jk}} &= \\color{red}{\\frac{\\partial E}{\\partial z^l_j}} \\color{blue}{\\frac{\\partial z^l_j}{\\partial w^l_{jk}}}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial b^l_{j}} &= \\color{red}{\\frac{\\partial E}{\\partial z^l_j}} \\color{blue}{\\frac{\\partial z^l_j}{\\partial b^l_{j}}}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 2;\">\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\color{blue}{\\frac{\\partial z^l_j}{\\partial w^l_{jk}}} = o^{l-1}_k\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\color{blue}{\\frac{\\partial z^l_j}{\\partial b^l_{j}}} = 1\n",
    "\\end{align*}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\delta^l_j &:= \\color{red}{\\frac{\\partial E}{\\partial z^l_j}} \\\\ &= \\frac{\\partial E}{\\partial o^l_j} \\frac{\\partial o^l_j}{\\partial z^l_j} \\\\\n",
    "&= \\sigma'(o^l_j) \\frac{\\partial E}{\\partial o^l_j} \\\\\n",
    "&= \n",
    "\\begin{cases}\n",
    "    \\sigma'(o^l_j) \\frac{\\partial E}{\\partial o^l_j}, & \\text{if}\\ l = N \\\\\n",
    "    \\sigma'(o^l_j) \\sum_i \\frac{\\partial E}{\\partial z^{l+1}_i} \\frac{\\partial z^{l+1}_i}{\\partial o^l_j}, & \\text{if}\\ l < N\n",
    "\\end{cases} \\\\\n",
    "&=\n",
    "\\begin{cases}\n",
    "    \\sigma'(o^l_j) (o^l_j - y^j), & \\text{if}\\ l = N \\\\\n",
    "    \\sigma'(o^l_j) \\sum_i \\delta^{l+1}_i w^{l+1}_{ij}, & \\text{if}\\ l < N\n",
    "\\end{cases} \\\\\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 2;\">\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\Delta w^l_{jk} &= - \\frac{\\eta}{M} \\color{red}{\\delta^l_j} \\color{blue}{o^{l-1}_k}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\Delta b^l_j &= - \\frac{\\eta}{M} \\color{red}{\\delta^l_j}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\delta^l_j = \n",
    "\\begin{cases}\n",
    "    \\sigma'(z^l_j) (o^l_j - y^j), & \\text{if}\\ l = N \\\\\n",
    "    \\sigma'(z^l_j) \\sum_i \\delta^{l+1}_i w^{l+1}_{ij}, & \\text{if}\\ l < N\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```C\n",
    "double delta(NetworkObject *self, int l, int j, double *output)\n",
    "{\n",
    "    if ( l < self->numLayers-1)\n",
    "    {\n",
    "        double sum = 0;\n",
    "        int i;\n",
    "        for ( i = 0; i < self->numNeurons[l+1]; i++ )\n",
    "        {\n",
    "            sum += delta(self, l+1, i, output) * self->layers[l+1].nodes[i].w[j];\n",
    "        }\n",
    "        return sum * self->activationFuncGradient(self->layers[l].nodes[j].o);\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        return self->activationFuncGradient(self->layers[l].nodes[j].o)\n",
    "               * (self->layers[l].nodes[j].o - output[j]);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Extending Python\n",
    "\n",
    "```C\n",
    "#include <Python.h>\n",
    "```\n",
    "* functions need to return pointer to a ``PyObject``\n",
    "```C\n",
    "static PyObject* Network_train(NetworkObject *self, PyObject *args)\n",
    "{\n",
    "    return Py_None;\n",
    "}\n",
    "```\n",
    "* parsing between Python and C types\n",
    "```C\n",
    "...\n",
    "if (! PyArg_ParseTupleAndKeywords(args, kwargs, \"O!lld\", kwlist, &PyList_Type,\n",
    "                              &listObj, &epochs, &batchsize, &eta))\n",
    "return NULL;\n",
    "```\n",
    "* Python method definitions, type definitions\n",
    "* module description, module initialization\n",
    "\n",
    "Full documentation available at https://docs.python.org/3/extending/extending.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building\n",
    "\n",
    "* Python script `setup.py` using `setuptools`\n",
    "* Compiler (on Windows): Microsoft Visual C++ (MSVC)\n",
    "* Compilation for Windows, Linux and macOS possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```Python\n",
    "from setuptools import setup, Extension\n",
    "\n",
    "module = Extension(\"pyceptron\",\n",
    "                   sources = ['pyceptron.c'],\n",
    "                   include_dirs=[],\n",
    "                   library_dirs=[],\n",
    "                   libraries=[])\n",
    "\n",
    "setup (name = \"pyceptron\",\n",
    "\t   version = \"0.1\",\n",
    "\t   description = \"Multi-layer perceptron for Python.\",\n",
    "\t   ext_modules = [module])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`python setup.py build_ext --inplace`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Demonstration: The MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* the MNIST database of handwritten digits can be downloaded at http://yann.lecun.com/exdb/mnist/\n",
    "* constructed from NIST's Special Database 1 and 3\n",
    "* 60,000 training samples, 10,000 test samples, written by high school students and Census Bureau employees\n",
    "* each set contains characters by disjoint groups of approx. 250 people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* it is presented in the ``idx`` file format\n",
    "```\n",
    "magic number\n",
    "size in dimension 1\n",
    "size in dimension 2\n",
    "size in dimension 3\n",
    "....\n",
    "size in dimension N\n",
    "data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "[offset] [type]          [value]          [description] \n",
    "0000     32 bit integer  0x00000803(2051) magic number \n",
    "0004     32 bit integer  60000            number of images \n",
    "0008     32 bit integer  28               number of rows \n",
    "0012     32 bit integer  28               number of columns \n",
    "0016     unsigned byte   0 - 255          pixel \n",
    "0017     unsigned byte   0 - 255          pixel \n",
    "........ \n",
    "xxxx     unsigned byte   0 - 255          pixel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"media/img/matrix_to_digit.png\" alt=\"drawing\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* ``mnist.py`` reads and converts MNIST data it into a usable format consisting of vectors with values ranging from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\left[4\\right]_{1} \\longrightarrow \\left[0, 0, 0, 0, 1, 0, 0, 0, 0, 0\\right]_{10}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\begin{bmatrix}\n",
    "0 & 2 & \\cdots & 0 \\\\\n",
    "1 & \\ddots & 243 & \\vdots \\\\\n",
    "\\vdots & 167 & \\ddots & 0 \\\\\n",
    "0 & \\cdots & 0 & 1 \\\\\n",
    "\\end{bmatrix}_{28\\times28} \\longrightarrow \\left[ 0, \\frac{2}{255}, \\cdots, 0, \\frac{1}{255}, \\cdots, \\frac{243}{255}, \\cdots, \\frac{167}{255}, \\cdots, 0, 0, \\cdots, 0, \\frac{1}{255}\\right]_{784}\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"modules\")\n",
    "import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "training_set = mnist.read(path='mnist', dataset='training')\n",
    "testing_set = mnist.read(path='mnist', dataset='testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Constructing and training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 784 input nodes, one per pixel\n",
    "* 10 output nodes, one per label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ReLU activation\n",
    "* softmax layer\n",
    "* stochastic gradient descent (batchsize = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"extension\")\n",
    "import pyceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "network = pyceptron.Network([784, 10], activation='ReLU', softmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that the network is set up, let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "network.train(training_set, epochs=1, batchsize=1, eta=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicting test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* predict a sample from the **testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prediction = network.predict(testing_set[42][0])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This seems to be a 4. Let's check by showing the actual image alongside with it's label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mnist.show(np.array(testing_set[42][0]).reshape(28, 28))\n",
    "print(testing_set[42][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Seems like the network was correct. But what is our actual error rate over the whole testing set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "num_wrong = 0\n",
    "for sample in testing_set:\n",
    "    prediction = network.predict(sample[0])\n",
    "    prediction = np.argmax(prediction)\n",
    "    expectation = np.argmax(sample[1])\n",
    "    if not prediction == expectation:\n",
    "        num_wrong += 1\n",
    "error_rate = num_wrong / len(testing_set)\n",
    "print(\"Achieved testing error rate: %.2f %%.\" % (error_rate * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Only 9 %? That's already beats the linear classifier by LeCun et al. 1998."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training for more epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Below is a graph illustrating the evolution of the testing and training errors when training for up to 50 epochs (using a learning rate of 0.005)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"media/plots/784_10_sgd_ReLU_softmax_807.png\" alt=\"drawing\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Just by evolving the network for a while we can achieve an error rate of 8.15 % for the testing set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By adding a hidden layer with 300 neurons we can lower this to < 3 %. However, this increases the time needed for training dramatically (~ 4 hours for 50 epochs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 3;\">\n",
    "<div style=\"width:33%; display: inline-block;\">\n",
    "<img src=\"media/plots/784_300_10_sgd_sigmoid_298.png\" alt=\"drawing\"/>\n",
    "    <center>Stochastic Gradient Descent, sigmoid</center>\n",
    "</div>\n",
    "\n",
    "<div style=\"width:33%; display: inline-block;\">\n",
    "<img src=\"media/plots/784_300_10_sgd_ReLU_softmax_229.png\" alt=\"drawing\"/>\n",
    "    <center>Stochastic Gradient Descent, ReLU, softmax</center>\n",
    "</div>\n",
    "\n",
    "<div style=\"width:33%; display: inline-block;\">\n",
    "<img src=\"media/plots/784_300_10_sgd_LeakyReLU_softmax_251.png\" alt=\"drawing\"/>\n",
    "    <center>Stochastic Gradient Descent, Leaky ReLU, softmax</center>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* lowest error: 2.29 % using ReLU with softmax layer\n",
    "* stochastic gradient descent convergest the fastest\n",
    "* random weight initialization is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The image below shows the 229 testing samples which the network wasn't able to predict correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![bad](media/img/bad.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Dataset: Basic Arithmetic Operators\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"media/img/collage.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Dataset\n",
    "* 220 samples from approx. 30 students\n",
    "* students were asked to write down the arithmetic operators +, -, *, /\n",
    "* no restrition on style (i.e. :, ÷, / were all allowed)\n",
    "\n",
    "## Training\n",
    "* 100 hidden units, ReLU, softmax\n",
    "* testing set was randomly chosen from dataset\n",
    "* 165 training samples, 55 testing samples\n",
    "* training for 50 epochs yielded testing error rates between 2 %  and 15 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "* simple algorithm can be very effective\n",
    "* memory management\n",
    "    * which values do we have to store\n",
    "    * can we reuse them later?\n",
    "* the network is very slow (100~300 images/s for 2-layer NN)\n",
    "    * code was written for readability first, performance second\n",
    "    * bottlenecks:\n",
    "        * conversion between C and Python types\n",
    "        * recursive backpropagation\n",
    "    * possible optimizations\n",
    "        * CPU parallelism\n",
    "        * GPU parallelism (e.g. CUDA)\n",
    "        * use of external libraries (e.g. NumPy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Writing a simple network might be easy, perfecting it is hard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "[1] Y. LeCun, L. Bottou, Y. Bengio and P. Haffner: Gradient-Based Learning Applied to Document Recognition, Proceedings of the IEEE, 86(11):2278-2324, November 1998<br>\n",
    "[2] Y.LeCun et al. The MNIST database of handwritten digits. available at http://yann.lecun.com/exdb/mnist/ (Jan. 2019)<br>\n",
    "[3] Eli Bendersky. The Softmax function and its derivative. available at https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/ (Jan. 2019)<br>\n",
    "[4] Michael Nielsen. Neural Networks and Deep Learning. free online book. available at http://neuralnetworksanddeeplearning.com/ (Jan. 2019)<br>\n",
    "[5] Paras Dahal. Classification and Loss Evaluation - Softmax and Cross Entropy Loss. available at https://deepnotes.io/softmax-crossentropy (Jan. 2019)<br>\n",
    "[6] Rob DiPietro. A Friendly Introduction to Cross-Entropy Loss. available at https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/ (Jan. 2019)<br>\n",
    "[7] Alex Kesling. mnist.py. available at https://gist.github.com/akesling/5358964 (Jan. 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## https://github.com/CaptainProton42/MNISTFromScratch"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
