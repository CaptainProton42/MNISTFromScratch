{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Semester Project: Do It Yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The goal of this project is to:\n",
    "* Write a machine learning algorithm from scratch\n",
    "* Apply this algorithm to a training set, in this case MNIST[1]\n",
    "* Examine how well the algorithm functions\n",
    "\n",
    "I expanded this scope a bit by also creating my own <i>MNIST-style</i> training set and applying the algorithm to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Neural Net: Simple and low error rates (< 2 %)[1] on the MNIST set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"media/img/neuralnet.png\" alt=\"drawing\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* sigmoid, tanh, ReLU or linear activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```C\n",
    "typedef struct {\n",
    "    PyObject_HEAD\n",
    "\n",
    "    Layer *layers; /* array of layer objects */\n",
    "\n",
    "    int numLayers; /* number of layers */\n",
    "    int *numNeurons; /* number of neurons in each layer */\n",
    "\n",
    "    double (*activationFunc)(double); /* activation function */\n",
    "    double (*activationFuncGradient)(double); /* gradient of activation function */\n",
    "\n",
    "    int softmax;    /* enable softmax? */\n",
    "} NetworkObject;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```C\n",
    "typedef struct layerStruct {\n",
    "    Node *nodes; /* Nodes in layer */\n",
    "\n",
    "    void (*initLayer)(struct layerStruct *self,\n",
    "                      int numberOfNodes,\n",
    "                      int numberOfPreviousNodes);\n",
    "} Layer;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```C\n",
    "typedef struct nodeStruct {\n",
    "    double o; /* Node output */\n",
    "    double z; /* Net Input */\n",
    "\n",
    "    double *w; /* Input weights */\n",
    "    double *grad_w; /* Input weight gradients */\n",
    "    double b; /* Input bias */\n",
    "    double grad_b; /* Input bias gradient */\n",
    "\n",
    "    void (*initNode)(struct nodeStruct *self, int numberOfPreviousNodes);\n",
    "} Node;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```C\n",
    "for ( sampleInBatch = 0; sampleInBatch < batchsize; sampleInBatch++ )\n",
    "{\n",
    "    sample = sampleInBatch + batch * batchsize; // batch offset\n",
    "    ...\n",
    "    forwardfeed(self);\n",
    "    if (self->softmax) softmax(self);\n",
    "    backpropagation(self, outputs[sample]);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward Feed\n",
    "```C\n",
    "void forwardfeed(NetworkObject *self)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "o_j^l = \\sigma \\left( \\sum_{k=0}^{n_{l-1} - 1} o_k^{l-1} w_{jk}^l + b_j^l \\right)\n",
    "= \\sigma \\left( \\mathbf{o^{l-1}} \\cdot \\mathbf{w_j^l} + b_j^l \\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```C\n",
    "int l, j;\n",
    "for ( l = 1; l < self->numLayers; l++ )\n",
    "{\n",
    "    for ( j = 0; j < self->numNeurons[l]; j++ )\n",
    "    {\n",
    "        /* net input */            \n",
    "        self->layers[l].z[j] = dot(self->layers[l].w[j],\n",
    "                                   self->layers[l-1].o,\n",
    "                                   self->numNeurons[l-1]);\n",
    "        self->layers[l].z[j] += self->layers[l].b[j];\n",
    "        self->layers[l].o[j] = self->activationFunc(self->layers[l].z[j]);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```C\n",
    "double (*activationFunc)(double);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 3;\">\n",
    "<div style=\"width:10%; display: inline-block;\">\n",
    "sigmoid:\n",
    "</div>\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma (x) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma'(x) = \\sigma(x) ( 1 - \\sigma(x))\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 2;\">\n",
    "<div style=\"width:10%; display: inline-block;\">\n",
    "tanh:\n",
    "</div>\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma (x) = \\frac{1 + \\tanh(z)}{2}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma'(x) = \\frac{1 - \\sigma(x)^2}{2}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 2;\">\n",
    "<div style=\"width:10%; display: inline-block;\">\n",
    "ReLU:\n",
    "</div>\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma (x) =\n",
    "\\begin{cases}\n",
    "    x, & \\text{if}\\ x > 0 \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma'(x) =\n",
    "\\begin{cases}\n",
    "    1, & \\text{if}\\ x > 0 \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 2;\">\n",
    "<div style=\"width:10%; display: inline-block;\">\n",
    "linear:\n",
    "</div>\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma (x) = x\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sigma'(x) = 1\n",
    "\\end{align*}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Squared Error Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The squared error loss is defined as\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "E = \\frac{1}{2} \\sum_k ( y_k - o_k )^2 \n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "where $\\mathbf{y}$ is our target and $\\mathbf{o}$ is our predicted output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The derivative is easy:\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial o_i} = o_i - y_i\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Softmax and Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Softmax normalizes an unnormalized vector into a probability distribution.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "p_j = \\frac{e^{o_j}}{\\sum_{k=0}^{n_j} e^{o_k}}\n",
    "\\end{align*}\n",
    "$\n",
    "for $j = 1,\\dots,n_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```C\n",
    "void softmax(NetworkObject *self)\n",
    "{\n",
    "    ...\n",
    "    for ( j = 0; j < self->numNeurons[l]; j++)\n",
    "    {\n",
    "        z += exp(self->layers[l].o[j]);\n",
    "    }\n",
    "\n",
    "    for ( j = 0; j < self->numNeurons[l]; j++)\n",
    "    {\n",
    "        self->layers[l].o[j] = exp(self->layers[l].o[j]) / z;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Entropy\n",
    "\n",
    "In information theory: Minimum number of bits needed to encode a known distribution (probability mass function) $y_i$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "H(y) = -\\sum_i y_i \\log(y_i)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Cross Entropy\n",
    "\n",
    "Number of bits needed to encode a probability mass function $y_i$ using an unoptimal distribution $p_i$.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "H(y,p) = - \\sum_i y_i \\log(p_i)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want to <b>minimize</b> this <b>loss function</b> so as to reach the optimal encoding $p_i = y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient of Cross Entropy Loss\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial H}{\\partial o_i} &= \\sum_k \\frac{\\partial H}{\\partial p_k} \\cdot \\frac{\\partial p_k}{\\partial o_i} \\\\\n",
    "&= -\\sum_k y_k \\frac{1}{p_k} \\cdot \\frac{\\partial p_k}{\\partial o_i}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using the derivative\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial p_k}{\\partial a_j} = p_k ( \\delta_{ki} - p_i )\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "of the softmax function we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial H}{\\partial o_i} = p_i - y_i\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "which is coincidentally also the loss gradient of the squared error function using the output of the softmax layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 2;\">\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial w^l_{jk}} &= \\color{red}{\\frac{\\partial E}{\\partial o^l_j} \\frac{\\partial o^l_j}{\\partial z^l_j}} \\color{blue}{\\frac{\\partial z^l_j}{\\partial w^l_{jk}}}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial b^l_{j}} &= \\color{red}{\\frac{\\partial E}{\\partial o^l_j} \\frac{\\partial o^l_j}{\\partial z^l_j}} \\color{blue}{\\frac{\\partial z^l_j}{\\partial b^l_{j}}}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 2;\">\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\color{blue}{\\frac{\\partial z^l_j}{\\partial w^l_{jk}} = o^{l-1}_k}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\color{blue}{\\frac{\\partial z^l_j}{\\partial b^l_{j}} = 1}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\color{red}{\\delta^l_j = \\frac{\\partial E}{\\partial o^l_j} \\frac{\\partial o^l_j}{\\partial z^l_j}} = \\sigma'(o^l_j) \\frac{\\partial E}{\\partial o^l_j} = \n",
    "\\begin{cases}\n",
    "    \\sigma'(z^l_j) (o^l_j - y^j), & \\text{if}\\ l = N \\\\\n",
    "    \\sigma'(z^l_j) \\sum_i \\delta^{l+1}_i w^{l+1}_{ij}, & \\text{if}\\ l < N\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"column-count: 2;\">\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\Delta w^l_{jk} &= - \\eta \\color{red}{\\delta^l_j} \\color{blue}{o^{l-1}_k}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>\n",
    "\n",
    "<div style=\"width:40%; display: inline-block;\">\n",
    "$\n",
    "\\begin{align*}\n",
    "\\Delta b^l_j &= - \\eta \\color{red}{\\delta^l_j}\n",
    "\\end{align*}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\delta^l_j = \n",
    "\\begin{cases}\n",
    "    \\sigma'(z^l_j) (o^l_j - y^j), & \\text{if}\\ l = N \\\\\n",
    "    \\sigma'(z^l_j) \\sum_i \\delta^{l+1}_i w^{l+1}_{ij}, & \\text{if}\\ l < N\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```C\n",
    "double delta(NetworkObject *self, int l, int j, double *output)\n",
    "{\n",
    "    if ( l < self->numLayers-1) {\n",
    "        double sum = 0;\n",
    "        int k;\n",
    "        for ( k = 0; k < self->numNeurons[l+1]; k++ )\n",
    "        {\n",
    "            sum += delta(self, l+1, k, output) * self->layers[l+1].w[k][j];\n",
    "        }\n",
    "        return sum * self->activationFuncGradient(self->layers[l].o[j]);\n",
    "    } else {\n",
    "        return self->activationFuncGradient(self->layers[l].o[j]) \n",
    "                                            * (self->layers[l].o[j] - output[j]);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[1] Y.LeCun et al.. The MNIST database of handwritten digits. available at http://yann.lecun.com/exdb/mnist/ (Jan. 2019)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
